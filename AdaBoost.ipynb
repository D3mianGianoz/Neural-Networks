{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1-XqinKLxD7S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from copy import deepcopy\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BoaHBpRxD7T"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We import the dataset from Hastie & Tibshirani book. \n",
        "This is an artificially generated binary classification problem. Labels are in the set $\\{-1,+1\\}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GyM3cfGBxD7U"
      },
      "outputs": [],
      "source": [
        "X,y = sklearn.datasets.make_hastie_10_2()\n",
        "X_train = X[0:8000,:]\n",
        "y_train = y[0:8000]\n",
        "X_test = X[8000:,:]\n",
        "y_test = y[8000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUGDvrm3xD7U"
      },
      "source": [
        "# Adaboost implementation\n",
        "\n",
        "Here we implement the Adaboost algorith. We shall assume that:\n",
        "- that the problem is a binary classification problem with labels in $\\{-1, +1\\}$.\n",
        "- that the weakModel can fit a weighted sample set by means of the call `weakModel.fit(X,y,sample_weight=w)` where `w` is a vector of length $n=|X|=|y|$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hWMsceZLxD7U"
      },
      "outputs": [],
      "source": [
        "class AdaBoost:\n",
        "    def __init__(self, weakModel, T):\n",
        "        self.T = T\n",
        "        self.weakModel = weakModel\n",
        "        self.alphas = []\n",
        "        self.classifiers = []\n",
        "        self.wl_errors = []\n",
        "        self.ens_errors = []\n",
        "\n",
        "    def trainWeakModel(self, X, y, w):\n",
        "        # Per come Ã¨ impostato scikitlearn\n",
        "        result = deepcopy(self.weakModel)\n",
        "        result.fit(X,y,sample_weight = w)\n",
        "        return result\n",
        "\n",
        "    def weightedError(y, y_, w):\n",
        "        # errors = (y != y_)\n",
        "        # return np.matmul(w, errors)\n",
        "        return np.sum(w[y != y_])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n = len(y)\n",
        "        w = np.ones(n) / n\n",
        "\n",
        "        ensamble_predictions = np.zeros(n)\n",
        "        print_step = np.max([1, self.T/100])\n",
        "\n",
        "        for t in range(self.T):\n",
        "            mt = self.trainWeakModel(X,y,w)\n",
        "            y_ = mt.predict(X)\n",
        "            #et = weightedError(y, y_, w)\n",
        "            et = np.sum(w[y != y_])\n",
        "\n",
        "            if et >= 0.5:\n",
        "                print(\"Error et>=0.5\")\n",
        "            \n",
        "            at = 0.5 * math.log((1 - et)/et)\n",
        "\n",
        "            self.classifiers.append(mt)\n",
        "            self.alphas.append(at)\n",
        "\n",
        "            # diamo peso agli esempi\n",
        "            w = w * np.exp(-at * y * y_)\n",
        "            # normalizzo (w_i > 0)\n",
        "            w = w / np.sum(w)\n",
        "\n",
        "            ensamble_predictions += at * y_\n",
        "            # guardiamo le etichette (booleane) e le sommiamo\n",
        "            ensamble_error = np.sum(np.sign(ensamble_predictions) != y) / n\n",
        "\n",
        "            self.wl_errors.append(et)\n",
        "            self.ens_errors.append(ensamble_error)\n",
        "\n",
        "            # Logging per monitorare l'apprendimento\n",
        "            if t > 1 and t % print_step == 0:\n",
        "                print(\"step:{} et:{:4f} error:{:.4f}\".format(t, et, ensamble_error))\n",
        "          \n",
        "        #self.alphas = self.alphas\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        result = np.zeros(len(X))\n",
        "        for t in range(self.T):\n",
        "            y_ = self.classifiers[t].predict(X)\n",
        "            result += self.alphas[t] * y\n",
        "\n",
        "        return np.sign(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR21OCYEHWmq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxfgaYt8HSSg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5kMlpoexD7V"
      },
      "source": [
        "# Testing with an SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbMD8nmWxD7V"
      },
      "source": [
        "Let us now see how our iplementation of AdaBoost performs on the dataset we loaded above. In this experiment we want the weak learning algorithm $\\mathcal{A}$ to be good, but not too much. An SVM with a polynomial kernel of degre 3 works fine for our needs.\n",
        "\n",
        "The SVC implementation provided by sklearn does not work well when weights are normalized. The following code simply \"denormalize\" weights befor calling into SVC implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6wr5z0I6xD7V"
      },
      "outputs": [],
      "source": [
        "    class SVC_:\n",
        "        def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
        "            self.svc = SVC(kernel=kernel, degree=degree)\n",
        "\n",
        "        def fit(self, X,y,sample_weight=None):\n",
        "            if sample_weight is not None:\n",
        "                sample_weight = sample_weight * len(X)\n",
        "\n",
        "            self.svc.fit(X,y,sample_weight=sample_weight)\n",
        "            return self\n",
        "\n",
        "        def predict(self, X):\n",
        "            return self.svc.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hluT_MDPxD7W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step:2 et:0.500000 error:0.4941\n",
            "step:3 et:0.500000 error:0.4941\n",
            "step:4 et:0.500000 error:0.4941\n",
            "step:5 et:0.500000 error:0.4941\n",
            "step:6 et:0.500000 error:0.4941\n",
            "step:7 et:0.500000 error:0.4941\n",
            "step:8 et:0.500000 error:0.4941\n",
            "step:9 et:0.500000 error:0.4941\n",
            "step:10 et:0.500000 error:0.4941\n",
            "step:11 et:0.500000 error:0.4941\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6a14b59b7bcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweakModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"poly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0madaboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweakModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-1415018cc792>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainWeakModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#et = weightedError(y, y_, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1415018cc792>\u001b[0m in \u001b[0;36mtrainWeakModel\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Per come Ã¨ impostato scikitlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweakModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/pyVenv/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/pyVenv/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "weakModel = SVC(kernel=\"poly\", degree=3)\n",
        "adaboost = AdaBoost(weakModel, 100)\n",
        "clf = adaboost.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5mgmqzSHTNF"
      },
      "outputs": [],
      "source": [
        "y_train_ = clf.predict(X_train)\n",
        "y_test_ = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLmrh6rVxD7W"
      },
      "outputs": [],
      "source": [
        "accuracy = (0.5 - y_train_.T * y_train/(2 * len(y_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mWblPMRGgnE"
      },
      "outputs": [],
      "source": [
        "ptl.plot(adaboost.wl_errors, label=\"$\\epsilon_t$)\n",
        "ptl.plot(adaboost.ens_errors, label=\"EnsError\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83pKagp1xD7W"
      },
      "source": [
        "# Testing on the weakest of the weak learners"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyp-oDx1xD7W"
      },
      "source": [
        "We now want to experiment with a VERY weak learner. The weak learner works as follows:\n",
        "\n",
        "- it creates a random linear model by generating the needed weight vector $\\mathbf{w}$ at random; each weight shall be sampled from U(-1,1);\n",
        "- it evaluates the weighted loss $\\epsilon_t$ on the given dataset and flip the linear model if $\\epsilon_t > 0.5$\n",
        "- at prediction time it predicts +1 if $\\mathbf{x} \\cdot \\mathbf{w} > 0$ it predicts -1 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "oGriaFJ2xD7X"
      },
      "outputs": [],
      "source": [
        "class RandomLinearModel:\n",
        "    def loss(self, y, y_, w):\n",
        "        return np.sum(w[y != y_])\n",
        "        \n",
        "    def fit(self,X,y,sample_weight=None):\n",
        "        self.w = (np.random.rand(X.shape[1]) - 0.5) * 2.0\n",
        "        y_ = self.predict(x)\n",
        "        if self.loss(y, y_, sample_weight) > 0.5:\n",
        "            self.w = - self.w\n",
        "\n",
        "\n",
        "    def predict(self,X):\n",
        "        return np.sing(np.matmul(X, self.w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uQ8WysPxD7X"
      },
      "source": [
        "Let us now learn an AdaBoost model using the RandomLinearModel weak learner printing every $K$ iterations the weighted error and the current error of the ensemble. Evaluate the training and test error of the final ensemble model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-72lPKPxD7X"
      },
      "outputs": [],
      "source": [
        "rs = RandomLinearModel()\n",
        "a = AdaBoost(rs,10000)\n",
        "a.fit(X_train,y_train)\n",
        "\n",
        "y_train_ = a.predict(X_train)\n",
        "y_test_ = a.predict(X_test)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "AdaBoost.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}