{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "\n",
    "# Gans are very sensitive to hyperparameters, choosing right parameters for\n",
    "# the optimizer can make the difference between learning and not learning.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate= 0.0005, beta_1=0.6)\n",
    "\n",
    "# Having bigger batch improves gradient stima\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few notable things:\n",
    "\n",
    "- we only need the X_train data\n",
    "- the labels are scaled to stay between -1 and 1 (using an hyperbolic tangent with the output layer seems to give better results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# usually we normalized with 255 but now we stay between \n",
    "X_train = X_train / 127.5 - 1.\n",
    "X_train = X_train.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC Gan Idea\n",
    "G_in = Input(shape=[z_dim])\n",
    "x = Dense(1000, activation=\"relu\")(G_in)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(784, activation=\"tanh\")(x)\n",
    "G_out = Reshape([28,28,1])(x)\n",
    "G = Model(G_in, G_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = Input(shape=[28,28,1])\n",
    "x = Flatten()(D_in)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "x = Dense(128, activation=\"sigmoid\")(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "D_out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "D = Model(D_in, D_out)\n",
    "\n",
    "# Let's keep under control few important metrics (tp and tn in particular)\n",
    "D.compile(loss=\"binary_crossentropy\", optimizer=optimizer, \n",
    "    metrics=[\"accuracy\",tf.keras.metrics.TruePositives(),tf.keras.metrics.TrueNegatives()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_in = Input(shape=[z_dim])\n",
    "gan_out = D(G(gan_in))\n",
    "gan = Model(gan_in, gan_out)\n",
    "\n",
    "# we obtain -1/2 E_z log D(G(z)) simply by flipping the labels\n",
    "# b_cross_entropy(y,k) = - [y log k + (1-y)log(1-k)]\n",
    "# with y=1 we obtain b_cross_ent =  - log D(G(z))\n",
    "# that is what we looking for\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model_2 (Functional)         (None, 28, 28, 1)         1021752   \n",
      "_________________________________________________________________\n",
      "model_3 (Functional)         (None, 1)                 233985    \n",
      "=================================================================\n",
      "Total params: 1,255,737\n",
      "Trainable params: 1,252,713\n",
      "Non-trainable params: 3,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we need to know the dimension of gan input\n",
    "# gaussian noise\n",
    "def generate_random_z(gan, num):\n",
    "    G =  gan.layers[1]\n",
    "    z_dim = G.layers[0].input.shape[1]\n",
    "    return np.random.normal(0, 1, size=[num, z_dim])\n",
    "\n",
    "# immagini reali\n",
    "def get_real_imgs(ds, num):\n",
    "    ndx = np.random.randint(0, len(ds), size=num)\n",
    "    return ds[ndx]\n",
    "\n",
    "# generami delle immagini\n",
    "def get_gen_imgs(gan, num):\n",
    "    noise = generate_random_z(gan=gan, num=num)\n",
    "    G = gan.layers[1]\n",
    "    return G.predict(noise)\n",
    "\n",
    "def show_img_samples(gan, step_no):\n",
    "    D = gan.layers[2]\n",
    "    imgs = get_gen_imgs(gan, 25)\n",
    "    pred_fake = D.predict(imgs) < 0.5\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(20,20))\n",
    "    cmaps = { True: \"hot\", False: \"gray\" }\n",
    "\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            imgno = i*5 + j\n",
    "            ax = axes[i,j]\n",
    "            ax.imshow(imgs[imgno], cmap=cmaps[pred_fake[imgno][0]])\n",
    "\n",
    "    plt.savefig(\"images/img_{}.png\".format(step_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "z = G.layers[0].input.shape[1]\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_train(ds, gan, batch_size):\n",
    "    D = gan.layers[2]\n",
    "    D.trainable = True\n",
    "    \n",
    "    # we are gonna use one of the tips & tricks as seen in the theory part\n",
    "    for i in range(5):\n",
    "        x_real = get_real_imgs(ds, batch_size) \n",
    "        x_gen = get_gen_imgs(gan, batch_size)\n",
    "\n",
    "        y_real = np.ones([batch_size])\n",
    "        y_gen = np.zeros([batch_size])\n",
    "\n",
    "        x = np.concatenate([x_real, x_gen])\n",
    "        y = np.concatenate([y_real, y_gen])\n",
    "\n",
    "        perfs = D.train_on_batch(x,y)\n",
    "\n",
    "    return perfs\n",
    "\n",
    "def G_train(ds, gan, batch_size):\n",
    "    D = gan.layers[2]\n",
    "    D.trainable = False\n",
    "    \n",
    "    # *2 because of loss function 1/2\n",
    "    zs = generate_random_z(gan, batch_size*2)\n",
    "    y = np.ones([batch_size*2])\n",
    "\n",
    "    return gan.train_on_batch(zs, y)\n",
    "\n",
    "def train(gan, ds, steps, batch_size=batch_size):\n",
    "    D = gan.layers[2]\n",
    "\n",
    "    for i in range(steps):\n",
    "\n",
    "        d_loss, d_acc, d_tp, d_tn = D_train(ds, gan, batch_size)\n",
    "        g_loss = G_train(ds, gan, batch_size)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"\\ri:{} G[l:{:0.3f}] D[l:{:0.3f} a:{:0.3f} +:{:0.3f} -:{:0.3f}]\"\n",
    "                .format(i,    g_loss,      d_loss,    d_acc,     d_tp,     d_tn), end=\"\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            show_img_samples(gan, i)\n",
    "\n",
    "    show_img_samples(gan, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:9590 G[l:0.644] D[l:0.637 a:0.598 +:114.000 -:39.000]"
     ]
    }
   ],
   "source": [
    "train(gan, X_train, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "43dc7463f0fcfaf697fb20fe523f592a92136fffc222679ff138d1a252a3d6ec"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}